{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsF9LZ1FVAUiLTLKvH1UCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanu1234566789/ad--nlp/blob/main/NLP_LAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eogbfGaM0pH",
        "outputId": "33845083-9d51-42aa-84c2-6dae47573728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.079*\"drive\" + 0.079*\"sister\" + 0.079*\"father\" + 0.056*\"pressur\" + 0.055*\"sugar\"')\n",
            "(1, '0.084*\"sugar\" + 0.080*\"lifestyl\" + 0.080*\"health\" + 0.080*\"expert\" + 0.080*\"good\"')\n",
            "Topic Assignments: [0, 0, 0, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "documents = [\n",
        "    \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\",\n",
        "    \"My father spends a lot of time driving my sister around to dance practice.\",\n",
        "    \"Doctors suggest that driving may cause increased stress and blood pressure.\",\n",
        "    \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\",\n",
        "    \"Health experts say that Sugar is not good for your lifestyle.\"\n",
        "]\n",
        "def preprocess(doc):\n",
        "    return preprocess_string(doc)\n",
        "preprocessed_documents = [preprocess(doc) for doc in documents]\n",
        "dictionary = corpora.Dictionary(preprocessed_documents)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
        "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "topic_assignments = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n",
        "print(\"Topic Assignments:\", topic_assignments)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "documents = [\n",
        "      \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Data privacy and security are important in modern technology.\",\n",
        "    \"Mobile apps are becoming more popular among users.\",\n",
        "    \"E-commerce platforms make online shopping convenient.\",\n",
        "    \"Cybersecurity measures are crucial to protect against cyber threats.\"\n",
        "]\n",
        "def preprocess(doc):\n",
        "    return preprocess_string(doc)\n",
        "preprocessed_documents = [preprocess(doc) for doc in documents]\n",
        "dictionary = corpora.Dictionary(preprocessed_documents)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
        "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "topic_assignments = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n",
        "print(\"Topic Assignments:\", topic_assignments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMQGf6puNByr",
        "outputId": "9403a262-90ba-4faf-8498-74372aafd9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.062*\"intellig\" + 0.062*\"subset\" + 0.062*\"artifici\" + 0.062*\"machin\" + 0.062*\"protect\"')\n",
            "(1, '0.053*\"privaci\" + 0.053*\"data\" + 0.053*\"secur\" + 0.053*\"modern\" + 0.053*\"technolog\"')\n",
            "Topic Assignments: [0, 1, 1, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import regex as re\n",
        "def file_to_sentence_list(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    sentences = [sentence.strip() for sentence in re.split(\n",
        "        r'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
        "\n",
        "    return sentences\n",
        "file_path ='/content/pizza.txt'\n",
        "text_data = file_to_sentence_list(file_path)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(total_words)\n",
        "input_sequences = []\n",
        "for line in text_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(\n",
        "    input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10,\n",
        "                    input_length=max_sequence_len-1))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "seed_text = \"Pizza have different \"\n",
        "next_words = 5\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences(\n",
        "        [token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted_probs = model.predict(token_list)\n",
        "    predicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
        "    seed_text += \" \" + predicted_word\n",
        "print(\"Next predicted words:\", seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMCLeUUmRR5p",
        "outputId": "25d4157f-3203-4b14-a295-b617f67261b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.0530 - loss: 6.3531\n",
            "Epoch 2/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.0598 - loss: 5.7801\n",
            "Epoch 3/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.0489 - loss: 5.7407\n",
            "Epoch 4/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.0711 - loss: 5.6225\n",
            "Epoch 5/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.0607 - loss: 5.6309\n",
            "Epoch 6/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.0548 - loss: 5.6083\n",
            "Epoch 7/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.0527 - loss: 5.5292\n",
            "Epoch 8/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.0700 - loss: 5.5050\n",
            "Epoch 9/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.0768 - loss: 5.4234\n",
            "Epoch 10/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.0724 - loss: 5.3335\n",
            "Epoch 11/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.0935 - loss: 5.3058\n",
            "Epoch 12/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.0924 - loss: 5.1881\n",
            "Epoch 13/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.0942 - loss: 5.0063\n",
            "Epoch 14/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.1030 - loss: 4.8077\n",
            "Epoch 15/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.1102 - loss: 4.7131\n",
            "Epoch 16/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.1267 - loss: 4.5546\n",
            "Epoch 17/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.1244 - loss: 4.4544\n",
            "Epoch 18/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.1547 - loss: 4.3427\n",
            "Epoch 19/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.1366 - loss: 4.2578\n",
            "Epoch 20/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.1479 - loss: 4.1367\n",
            "Epoch 21/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.1751 - loss: 3.9859\n",
            "Epoch 22/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.1532 - loss: 3.9460\n",
            "Epoch 23/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.1790 - loss: 3.8293\n",
            "Epoch 24/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.1625 - loss: 3.8071\n",
            "Epoch 25/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.1802 - loss: 3.6907\n",
            "Epoch 26/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.2154 - loss: 3.5299\n",
            "Epoch 27/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.2137 - loss: 3.5346\n",
            "Epoch 28/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.2087 - loss: 3.3747\n",
            "Epoch 29/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.2158 - loss: 3.3622\n",
            "Epoch 30/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.2448 - loss: 3.2617\n",
            "Epoch 31/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.2855 - loss: 3.1192\n",
            "Epoch 32/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.3196 - loss: 3.0103\n",
            "Epoch 33/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.3180 - loss: 2.9399\n",
            "Epoch 34/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.3305 - loss: 2.9249\n",
            "Epoch 35/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.3516 - loss: 2.8107\n",
            "Epoch 36/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.3755 - loss: 2.7664\n",
            "Epoch 37/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4070 - loss: 2.6497\n",
            "Epoch 38/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4345 - loss: 2.6354\n",
            "Epoch 39/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.4502 - loss: 2.5261\n",
            "Epoch 40/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.4830 - loss: 2.4159\n",
            "Epoch 41/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.5155 - loss: 2.3643\n",
            "Epoch 42/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5301 - loss: 2.2972\n",
            "Epoch 43/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5571 - loss: 2.2515\n",
            "Epoch 44/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.5801 - loss: 2.1547\n",
            "Epoch 45/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.6005 - loss: 2.1015\n",
            "Epoch 46/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6044 - loss: 2.0318\n",
            "Epoch 47/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6403 - loss: 1.9626\n",
            "Epoch 48/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.6371 - loss: 1.8934\n",
            "Epoch 49/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.6672 - loss: 1.8164\n",
            "Epoch 50/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6656 - loss: 1.8274\n",
            "Epoch 51/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.6755 - loss: 1.7305\n",
            "Epoch 52/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.6803 - loss: 1.7077\n",
            "Epoch 53/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.7207 - loss: 1.6281\n",
            "Epoch 54/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7228 - loss: 1.5477\n",
            "Epoch 55/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7295 - loss: 1.5059\n",
            "Epoch 56/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7453 - loss: 1.4952\n",
            "Epoch 57/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7596 - loss: 1.4270\n",
            "Epoch 58/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.7645 - loss: 1.3709\n",
            "Epoch 59/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7767 - loss: 1.3504\n",
            "Epoch 60/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7741 - loss: 1.3508\n",
            "Epoch 61/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7971 - loss: 1.2587\n",
            "Epoch 62/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7962 - loss: 1.2552\n",
            "Epoch 63/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.8202 - loss: 1.1858\n",
            "Epoch 64/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8205 - loss: 1.1541\n",
            "Epoch 65/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8059 - loss: 1.1844\n",
            "Epoch 66/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8256 - loss: 1.0970\n",
            "Epoch 67/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8373 - loss: 1.0409\n",
            "Epoch 68/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8464 - loss: 1.0059\n",
            "Epoch 69/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.8416 - loss: 1.0029\n",
            "Epoch 70/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8608 - loss: 0.9440\n",
            "Epoch 71/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.8703 - loss: 0.9071\n",
            "Epoch 72/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8666 - loss: 0.9055\n",
            "Epoch 73/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8684 - loss: 0.8860\n",
            "Epoch 74/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.8715 - loss: 0.8658\n",
            "Epoch 75/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8930 - loss: 0.8003\n",
            "Epoch 76/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8870 - loss: 0.7947\n",
            "Epoch 77/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9066 - loss: 0.7425\n",
            "Epoch 78/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.9006 - loss: 0.7340\n",
            "Epoch 79/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9061 - loss: 0.7054\n",
            "Epoch 80/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8988 - loss: 0.7122\n",
            "Epoch 81/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9258 - loss: 0.6675\n",
            "Epoch 82/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9188 - loss: 0.6505\n",
            "Epoch 83/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.9086 - loss: 0.6395\n",
            "Epoch 84/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9097 - loss: 0.6418\n",
            "Epoch 85/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9296 - loss: 0.5863\n",
            "Epoch 86/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9172 - loss: 0.5850\n",
            "Epoch 87/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9248 - loss: 0.5736\n",
            "Epoch 88/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.9205 - loss: 0.5680\n",
            "Epoch 89/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9306 - loss: 0.5357\n",
            "Epoch 90/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.9328 - loss: 0.5164\n",
            "Epoch 91/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.9283 - loss: 0.5166\n",
            "Epoch 92/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9316 - loss: 0.4886\n",
            "Epoch 93/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.9383 - loss: 0.4752\n",
            "Epoch 94/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9368 - loss: 0.4700\n",
            "Epoch 95/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.9345 - loss: 0.4634\n",
            "Epoch 96/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9471 - loss: 0.4166\n",
            "Epoch 97/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.9516 - loss: 0.4028\n",
            "Epoch 98/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.9380 - loss: 0.4190\n",
            "Epoch 99/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9495 - loss: 0.3896\n",
            "Epoch 100/100\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9500 - loss: 0.3865\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Next predicted words: Pizza have different  become a symbol of comfort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZVPhZsQa7YD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}